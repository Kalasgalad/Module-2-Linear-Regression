{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0754bfe3-b3f5-40ce-a2ef-81309abe7332",
   "metadata": {},
   "source": [
    "# Regression Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f35e9c-4195-47c2-a5a7-42ce0d6d6a01",
   "metadata": {},
   "source": [
    "**Using a dataset found on rentfaster.ca, I will be analyzing the market for rental prices across Canada in 2024.** <br>\n",
    "**The goal of this analysis is to predict, using different regression models, the rental price most accurately.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259db23-c0bc-44bb-add4-b33e9d0ae1a8",
   "metadata": {},
   "source": [
    "## Project Outline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6a7276-9457-4d97-8d71-c79f24613887",
   "metadata": {},
   "source": [
    "1. The project will begin with an EDA portion to better understand the data, indentify trends, and determine important features useful for regression models.\n",
    "2. Data will be cleaned and prepared for visualization.\n",
    "3. Once data has been organized and cleaned, visualization will be included to support my analysis of aforementioned trends. Key variables will be determined for feature engineering.\n",
    "4. Proceed with feature engineering to formulate three(3) regression models.\n",
    "5. Compare different regression models & distinguish which will best predict prices.\n",
    "6. Use final model to make new predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd1402c8-02fa-45c0-a35e-c2225bf72116",
   "metadata": {},
   "source": [
    "### 1. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd9865d1-9b1f-4467-addd-65fa452846ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a10717e6-bc43-4cbe-8439-0ef65e36e8c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/AlexandreRioux/Desktop/M2P07-Regression_Project-main/canada_rent.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Loading dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m rental_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/AlexandreRioux/Desktop/M2P07-Regression_Project-main/canada_rent.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m rental_df\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/AlexandreRioux/Desktop/M2P07-Regression_Project-main/canada_rent.csv'"
     ]
    }
   ],
   "source": [
    "# Loading dataset\n",
    "rental_df = pd.read_csv('/Users/AlexandreRioux/Desktop/M2P07-Regression_Project-main/canada_rent.csv')\n",
    "rental_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e29f67d-b56f-4052-8312-90a8142d70cc",
   "metadata": {},
   "source": [
    "*The dataset is quite extensive with many columns describing the rentals.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917c7740-7b23-442e-b420-1bfe441b9b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploring\n",
    "print(rental_df.info())\n",
    "print(rental_df.describe())\n",
    "print(rental_df.sample(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbfc898-a270-4cb4-8b4f-4e8100493154",
   "metadata": {},
   "source": [
    "**Directly from dataset info, it is known that there are quite a few columns that have null values, specifically 'smoking' and 'sq feet' columns.** <br>\n",
    "**At first glance, 'latitude' and 'longitude' columns do not seem to be necessarily useful information for price prediction since city column exists. Although there are fluctuating costs attached to various neighbourhoods in one city, collecting data in the city would be more relevant.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88a2c2d-7245-4ad2-bba2-e50a1bf74817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's observe missing values\n",
    "print(rental_df.isnull().sum())\n",
    "print('----------------')\n",
    "# Let's observe data types\n",
    "print(rental_df.dtypes)\n",
    "print('----------------')\n",
    "# Let's observe the number of unique entries in each column\n",
    "print(rental_df.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d890fd-92e4-4422-8870-9a7b73d5cd25",
   "metadata": {},
   "source": [
    "From this first exploration, it might be prudent to keep latitude and longitude since they are numerical values that can easily be used in regression models. <br>\n",
    "Here are the changes I am moving forward with:<br>\n",
    "**Columns to remove:** <br>\n",
    "*link, address and rentfaster_id*<br>\n",
    "**Columns to keep as numerical data:** <br>\n",
    "*latitude, longitude, beds, bath, sq_feet*<br>\n",
    "**Columns to convert into numerical data:** <br>\n",
    "*lease_term, type, furnishing, availability date, smoking, cats, dogs*<br>\n",
    "\n",
    "The **price** will be the target variable used for prediction.<br>\n",
    "The square feet column will be averaged out according to beds, baths and price."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8751412-cc65-4a58-a0ff-f94a1fae05e2",
   "metadata": {},
   "source": [
    "## 2. Cleaning and Preparing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fd7f67-43d1-4f66-adba-6420b2c86eb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning Data\n",
    "clean_rental_df = rental_df.drop(columns=['rentfaster_id','address', 'link'])\n",
    "clean_rental_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a17ac1-4337-4f17-9f59-62822e2070e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using mode operations because mean & median would give values that are not realistic to homes.\n",
    "\n",
    "# Finding mode in 'beds' column\n",
    "mode_beds = clean_rental_df['beds'].mode()[0]\n",
    "mode_beds # Mode is 2 for 'beds'\n",
    "# Filling missing values in 'beds' column with the mode.\n",
    "clean_rental_df['beds'] = clean_rental_df['beds'].fillna(mode_beds)\n",
    "# Finding mode in 'baths' column\n",
    "mode_baths = clean_rental_df['baths'].mode()[0]\n",
    "mode_baths # Mode is 1 for 'baths'\n",
    "# Fillling mode in 'baths' column\n",
    "clean_rental_df['baths'] = clean_rental_df['baths'].fillna(mode_baths)\n",
    "# Finding mode in the 'lease_term' column\n",
    "mode_lease_term = clean_rental_df['lease_term'].mode()[0]\n",
    "mode_lease_term # Mode of 'lease_term' is 'Long Term'. Usually means 12 months.\n",
    "# Converting Long Term values as 12 months.\n",
    "clean_rental_df['lease_term'] = clean_rental_df['lease_term'].replace('Long Term', 12).fillna(12)\n",
    "# Renaming 'lease_term' column for clarification\n",
    "clean_rental_df = clean_rental_df.rename(columns={'lease_term': 'lease_term(months)'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532a4e3-26c2-4e20-9232-acec3487759f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing Negotiable values 12 (months).\n",
    "clean_rental_df['lease_term(months)'] = clean_rental_df['lease_term(months)'].replace('Negotiable', 12)\n",
    "# Changing Short Term values to 6 (months).\n",
    "clean_rental_df['lease_term(months)'] = clean_rental_df['lease_term(months)'].replace('Short Term', 6).fillna(6)\n",
    "# Changing Months values to 12 (months).\n",
    "clean_rental_df['lease_term(months)'] = clean_rental_df['lease_term(months)'].replace('months', 12).fillna(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534adba7-eee0-4989-94fd-651ac27714b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns with missing values in 'availability_date'\n",
    "clean_rental_df.dropna(subset=['availability_date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aba9d87-c923-4a50-b38e-1e2fe4440f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding mode and filling missing values in the 'smoking' column\n",
    "mode_smoking = clean_rental_df['smoking'].mode()[0]\n",
    "mode_smoking # Mode in Non-Smoking\n",
    "clean_rental_df['smoking'] = clean_rental_df['smoking'].fillna(mode_smoking)\n",
    "# Finding mode and filling missing values in the 'cats' and 'dogs' column\n",
    "mode_cats = clean_rental_df['cats'].mode()[0]\n",
    "mode_cats # Mode is 'True' which means they are allowed\n",
    "mode_dogs = clean_rental_df['dogs'].mode()[0]\n",
    "mode_dogs # Mode is 'True' which means they are allowed\n",
    "clean_rental_df['cats'] = clean_rental_df['cats'].fillna(mode_cats)\n",
    "clean_rental_df['dogs'] = clean_rental_df['dogs'].fillna(mode_dogs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ad1b6-a2ae-4145-bfe4-edfa000d3777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up missing values in the 'sq_feet' column\n",
    "clean_rental_df.isna().sum()\n",
    "# Keep all numerical values, make the rest NaN values.\n",
    "def clean_sq_feet(value): # Creating function to convert values into strings\n",
    "    value_str = str(value)\n",
    "    if all(s.isdigit() or s.isspace() or s == '.' for s in value_str):\n",
    "        try:\n",
    "            return float(value_str.replace(\" \", \"\"))\n",
    "        except ValueError:\n",
    "            return np.nan\n",
    "    return np.nan\n",
    "clean_rental_df['sq_feet'] = clean_rental_df['sq_feet'].apply(clean_sq_feet)\n",
    "# sq_feet column now only contains numerical values -> float.\n",
    "# Replace NaN values with the mean of numerical values.\n",
    "mean_sq_feet = clean_rental_df['sq_feet'].mean()\n",
    "mean_sq_feet # Mean is 885.64 square feet\n",
    "# Filling values\n",
    "clean_rental_df['sq_feet'] = clean_rental_df['sq_feet'].fillna(mean_sq_feet)\n",
    "print(clean_rental_df.isna().sum()) # No more null values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5be26e-40d5-4f4d-b7b9-e77a323953cd",
   "metadata": {},
   "source": [
    "## 3. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae430812-6f96-411a-8b56-36d82f7cffc3",
   "metadata": {},
   "source": [
    "#### Let's visualize the collected data to identify trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9387274-87b5-4940-8231-dd3472c52712",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_rental_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7ce5e5-98fe-42dc-8d78-a15d76df295e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's visualize the price vs. type of home\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=clean_rental_df, x='type', y='price', hue='type', palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Price ($)', fontsize=12)\n",
    "plt.xlabel('Type of Rental Unit', fontsize=12)\n",
    "plt.title('Type of Home vs. Price($)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dcf3ce5-9680-453c-bd87-62e60daaae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Price vs. Province\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=clean_rental_df, x='province', y='price', hue='province', palette='viridis')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Price ($)', fontsize =12)\n",
    "plt.xlabel('Province', fontsize=12)\n",
    "plt.title('Province vs. Price($)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Ontario, Vancouver and Quebec contain the highest prices for rental units.\n",
    "# Seeing as the 3 largest cities in Canada are in those provinces, this is expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cef59b-22d0-472f-908e-6389da475b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Price vs. Smoking\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=clean_rental_df, x='smoking', y='price', hue='smoking', palette='hls')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylabel('Price ($)', fontsize =12)\n",
    "plt.xlabel('Smoking Units', fontsize=12)\n",
    "plt.title('Smoking Units vs. Price($)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# As expected, non-smoking units are more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78999da-eca3-491f-a96f-5143bc6ff3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Price vs Furnished\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(data=clean_rental_df, x='furnishing', y='price', hue='furnishing')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylabel('Price ($)', fontsize =12)\n",
    "plt.xlabel('Square Feet', fontsize=12)\n",
    "plt.title('Furnished Rental Units vs. Price($)', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# As expected, furnished units are typically more expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b9f8fb-7567-470e-a6d8-b4425e09a5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of Square Footage vs. Price\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.scatterplot(data=clean_rental_df, x='sq_feet', y='price', color='red')\n",
    "plt.title('Rental Unit Square Footage vs. Price ($)', fontsize=16)\n",
    "plt.ylabel('Price ($)')\n",
    "plt.xlabel('Square Footage')\n",
    "plt.show()\n",
    "# There are only a few outliers in this dataset. They may influence feature engineering later on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73fab63-7ba8-4a5f-ab14-6d81a02b007d",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Regression Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9867844a-38d5-4bdc-a977-83464870b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding columns for modelling\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "# Converting categorical columns into numerical\n",
    "clean_rental_df.dtypes\n",
    "# Creating an instance of OneHotEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "encoder = OneHotEncoder(drop='first', sparse_output = False)\n",
    "# Transforming objects into floats for regression modelling\n",
    "encoded_cols = encoder.fit_transform(clean_rental_df[['city', 'province', 'type',\n",
    "                                                      'beds', 'baths', 'furnishing', 'availability_date', 'smoking', 'cats', 'dogs']])\n",
    "encoded_cols_clean_rental_df = pd.DataFrame(encoded_cols, columns = encoder.get_feature_names_out([\n",
    "    'city', 'province', 'type', 'beds', 'baths', 'furnishing', 'availability_date', 'smoking', 'cats', 'dogs']))\n",
    "encoded_rental_df = clean_rental_df.drop(columns=['city', 'province', 'type', 'beds', 'baths', 'furnishing', 'availability_date', \n",
    "                                                  'smoking', 'cats', 'dogs']).join(encoded_cols_clean_rental_df)\n",
    "encoded_rental_df['lease_term(months)'].unique() # lease_term(months) still has objects\n",
    "encoded_rental_df['lease_term(months)'] = encoded_rental_df['lease_term(months)'].replace({'12 months': 12, '6 months': 6})\n",
    "encoded_rental_df = encoded_rental_df.astype({'lease_term(months)': 'float64'})\n",
    "encoded_rental_df.dtypes # All features are floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cd79bf-a735-41a8-8cce-5051082cb57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing data for modelling using 'Price' as the target variable.\n",
    "# Importing train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Searching for features with the highest correlation to target variable\n",
    "encoded_rental_df.corr()\n",
    "corr_encoded_rental_df = encoded_rental_df.corr()\n",
    "# Obtaining columns with the highest correlation to 'price'\n",
    "price_corr_df = corr_encoded_rental_df['price']\n",
    "# Display top 10 columns\n",
    "top_10_corr = price_corr_df.drop('price').sort_values(ascending=False).head(10)\n",
    "print(top_10_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4033943-259f-4ccd-8237-50edeef0e27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing NaNs\n",
    "encoded_rental_df = pd.get_dummies(clean_rental_df, drop_first=False, dummy_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f1e550-5697-4afd-8ad5-5cc89bb20f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting features among the top correlations.\n",
    "# Omitting baths_7.5 & beds_8\n",
    "X = encoded_rental_df[['sq_feet', 'city_Toronto', 'province_Ontario', 'type_House', 'longitude', 'baths_2', 'city_Vancouver', 'baths_3.5']]\n",
    "y = encoded_rental_df['price']\n",
    "print('These are the features of the dataframe')\n",
    "print(X)\n",
    "print('This is the target variable')\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81487fe2-df7f-4108-8eb0-5c3fe8189e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making sure there are no more NaN values\n",
    "print(encoded_rental_df.isna().sum())\n",
    "encoded_rental_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0817c074-a3d9-4035-a4d6-177f65f70237",
   "metadata": {},
   "source": [
    "### 4.1 Regression Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42dc7eb-1640-4456-9210-70f936275a32",
   "metadata": {},
   "source": [
    "### *First Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392f1104-3b9e-4425-a869-b792ca98dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=1)\n",
    "print('This is the X_train array')\n",
    "print(X_train)\n",
    "print('This is the X_test array')\n",
    "print(X_test)\n",
    "print('This is the y_train array')\n",
    "print(y_train)\n",
    "print('This is the y_test array')\n",
    "print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fbe83c-22a6-43d1-b21e-3da88937d0fd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Importing StandardScaler\n",
    "# Because there are a few far-reaching outliers, I have chosen to go with this method to minimize the impacts they could have.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit transform on the training data\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "# Transform on the testing data\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "print('This is the scaled X_train darray')\n",
    "print(X_train)\n",
    "print('This is the scaled X_test array')\n",
    "print(X_test)\n",
    "\n",
    "# Importing model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr_model = LinearRegression()\n",
    "X_train.shape # Output is (21895, 8)\n",
    "y_train.shape # Output is (21895, 8)\n",
    "# Initializing training model\n",
    "lr_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1efacb6-a1a4-4751-b853-56b4e35d3fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing to calculate validity of model\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "y_pred = lr_model.predict(X_test)\n",
    "# Find Mean Absolute Error\n",
    "mae_lr = mean_absolute_error(y_test, y_pred)\n",
    "mse_lr = mean_squared_error(y_test, y_pred)\n",
    "rmse_lr = np.sqrt(mse_lr)\n",
    "r2_lr = r2_score(y_test, y_pred)\n",
    "\n",
    "print('This is the Mean Absolute Error')\n",
    "print(mae_lr)\n",
    "print('This is the Mean Squared Error')\n",
    "print(mse_lr)\n",
    "print('This is the RMSE')\n",
    "print(rmse_lr)\n",
    "print('This is the R2')\n",
    "print(r2_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ec1a5a-c46a-4fba-8db2-84bdda61a83e",
   "metadata": {},
   "source": [
    "### *Second Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d2a456-f3ab-482a-912a-0af156ec72ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Polynomial Regression Model\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly_converter = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# Fitting converter\n",
    "poly_features = poly_converter.fit_transform(X)\n",
    "poly_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a17fa-3b6b-4b18-b094-293375f8d313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Creating model\n",
    "poly_model = LinearRegression()\n",
    "# Fitting the training model\n",
    "poly_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d646c4-417c-4a02-8af5-1e8c9087f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating validity of model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(poly_features, y, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae67d40-22f3-4692-b30c-27de81e56355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing model\n",
    "y_pred = poly_model.predict(X_test)\n",
    "from sklearn.metrics import mean_absolute_error,mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "print('This is the Mean Absolute Error:', poly_mae)\n",
    "poly_mae = mean_absolute_error(y_test, y_pred)\n",
    "print('This is the Mean Squared Error:', poly_mse)\n",
    "poly_mse = mean_squared_error(y_test, y_pred)\n",
    "print('This is the RMSE:', poly_rmse)\n",
    "poly_rmse = np.sqrt(poly_mse)\n",
    "print('This is the R2:', poly_r2)\n",
    "poly_r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2144fd-8f68-4556-a463-5406797b480d",
   "metadata": {},
   "source": [
    "### *Third Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7dd48f-929e-4156-baa4-c9da3527a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using ElasticNetCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Initializing PolynomialFeatures\n",
    "poly_transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "# Fitting converter to features\n",
    "X_poly_features = poly_transformer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9988c6-2cea-4b65-8894-2d38250c6e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ElasticCV - Train_Test_Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly_features, y, test_size=0.15, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3faae7aa-a38c-43c2-98fd-c0cf8501c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_test= scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951aa5fa-acde-4aee-8e37-c7a12d72cbb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing ElasticCV\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "# Creating ElasticCV model\n",
    "elastic_model = ElasticNetCV(l1_ratio=[.1, .2, .5,.7, .90, .95, 1], eps=0.01, n_alphas=100, max_iter=15000)\n",
    "elastic_model.fit(scaled_X_train, y_train)\n",
    "elastic_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94249ebc-5715-4747-8bc7-072c18c0e4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Searching for best Alpha\n",
    "print(elastic_model.alpha_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eedb14-3d77-4d38-8a92-c86cdef2066a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating model\n",
    "y_pred = elastic_model.predict(scaled_X_test)\n",
    "\n",
    "elastic_mae = mean_absolute_error(y_test, y_pred)\n",
    "elastic_mse = mean_squared_error(y_test, y_pred)\n",
    "elastic_rmse = np.sqrt(elastic_mse)\n",
    "elastic_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('This is the Elastic Mean Absolute Error:', elastic_mae)\n",
    "print('This is the Elastic Mean Squared Error:', elastic_mse)\n",
    "print('This is the Elastic RMSE:', elastic_rmse)\n",
    "print('This is the Elastic R2:', elastic_r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f765dea-2377-4f30-9e4c-de0e75776520",
   "metadata": {},
   "source": [
    "## 5. Comparing the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad1f8fd-a7fb-459c-8b8a-32b51df0f758",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_comparison = {\n",
    "    'Models': ['Linear Regression (LR)', 'Polynomial Regression (PR)', 'Poly. Reg. with ElasticNetCV'],\n",
    "    'MAE': [mae_lr, poly_mae, elastic_mae],\n",
    "    'MSE': [mse_lr, poly_mse, elastic_mse],\n",
    "    'RMSE': [rmse_lr, poly_rmse, elastic_rmse],\n",
    "    'R²': [r2_lr, poly_r2, elastic_r2]}\n",
    "metrics_comparison_df = pd.DataFrame(metrics_comparison)\n",
    "metrics_comparison_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa767ef-8686-47b9-9419-d8b2196994fd",
   "metadata": {},
   "source": [
    "#### *Because Polynomial Regression (PR) has the lest amount of error and the highest r2 score, I will be choosing this for my final prediction model*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f579a-043f-4d55-ae9a-1f11251a982c",
   "metadata": {},
   "source": [
    "# 6. Final Model Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69699ad-78c6-438f-8f42-a2abffcf3a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the features of the final model (again)\n",
    "X = encoded_rental_df[['sq_feet', 'city_Toronto', 'province_Ontario', 'type_House', 'longitude', 'baths_2', 'city_Vancouver', 'baths_3.5']]\n",
    "y = encoded_rental_df['price']\n",
    "\n",
    "# Creating polynomial features by conversion\n",
    "poly_converter = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly_converter.fit_transform(X)  # Transforming the entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fed6e2-6def-4c43-90ef-897d89a51ab9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
